---
title: "FIFA Player position classification and market value estimation"
author: "Giovanni Dal Mas"
date: "null"
output:
  html_document:
    df_print: paged
  latex_engine: xelatex
  pdf_document: default
editor_options:
  markdown:
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Player position classification and market value estimation

## 1. Abstract and motivation

FIFA is one of the most known videogames and the most famous sport title in the industry, in particular we considered FIFA 22 edition.

Each player covers a specific position on the field; what we want to do in the first part of the project is building some models to classify the position of the player, based on the values of its attributes.
It's important to consider that some players may share some features with footballers playing in another position, and this may influence our task.
For example, some attacking midfielders (CAM) have a good shot and pace, just like wingers (RW, LW).
We will keep this into account and adjust our classification accordingly.

In the second part of our project we will perform a regression task, building some models to evaluate the market price of a football player, using Ridge and Lasso regression.

## 2. The dataset - Description & EDA

The original dataset has been extracted from <https://sofifa.com/> and contains 19239 players described by 110 different features.

### 2.1 DataFrame inspection and rough slicing

```{r,echo=FALSE}
#first we import libraries, some of which we never use, but just in case
library(knitr) 
library(ggplot2)    #graphs
library(reshape2)   #dataframe reshaping
library(viridis)    #colors
library(stringr)    #working with strings
library(FactoMineR) #factor analysis (PCA)
library(factoextra) #factor analysis (PCA)
library(stringr)

library(dplyr)
library(ggplot2)
library(MASS)
library(class)
library(gmodels)

library(tidyverse)
library(stringr)
library(corrplot)
library(gridExtra)
library(reshape)
library(corrplot)
library(caret)
library(randomForest)
library(cvms) 
library(e1071)
library(leaps)
library(data.table)
library(leaps)
library(glmnet)
```

We set the seed for reproducible experiments

```{r}
set.seed(42)
```

First we load the dataset, and check the dimension.

```{r}
players_full <- read.csv("players_22.csv") #full dataframe
dim(players_full) #full dataset
```

We have more or less 20k players with 110 attributes.
Below we look at how those attributes are named.

```{r}
colnames(players_full)
```

To get a better general idea, we also want to look at the type of data they provide

```{r}
head(players_full, 10)
```

We perform a rough removal of all the features that will obviously not be relevant to our classification, or some of the ones that are a obvious linear composition of other features.
Moreover, our training will be performed on the league 1 players.
Then, we check the dimensions again.

```{r}

players_full <- players_full[players_full$league_level == 1,]

players_22 <- subset(players_full, select = c("short_name","player_positions","age","potential", "international_reputation", "value_eur", "height_cm","weight_kg","pace","shooting","passing","preferred_foot","weak_foot","dribbling","defending","physic","attacking_crossing","attacking_finishing","attacking_heading_accuracy","attacking_short_passing","attacking_volleys","skill_dribbling","skill_curve","skill_fk_accuracy","skill_long_passing","skill_ball_control","movement_acceleration","movement_sprint_speed","movement_agility","movement_reactions","movement_balance","power_shot_power","power_jumping","power_stamina","power_strength","power_long_shots","mentality_aggression","mentality_interceptions","mentality_positioning","mentality_vision","mentality_penalties","mentality_composure","defending_marking_awareness","defending_standing_tackle","defending_sliding_tackle"))



dim(players_22)

```

Apparently we kept only 45 features.
Good enough.
We will remove more later by performing feature selection so stay tuned.

We have a short look at the numerical summary of all the features we selected.
On a first glance they look like they need some normalization.
But before that, we would love to make some visual presentations.

```{r}
summary(players_22)
```

### 2.2 Managing empty entries

We look at how many NAs we have on each attribute, in order to decide if we prefer removing them or filling them.

```{r}
which(apply(X = players_22, MARGIN = 2, FUN = anyNA) == TRUE) # check for NA
```

We decide that we have a statistically dispensable number of NAs so we remove them.

```{r}
players <- na.omit(players_22) # delete NA
dim(players)
```

We still have a good chunk of the dataset left.
Since goalkeepers have special stats, we also would like to take them out.
Given the fact that they are not movement players they may be missing one or more attributes typical of a footballer that plays outside, such shot precision, dribbling or finishing, for this reason they may have already been discarded in the previous phase.
Just to be sure, let's verify.

```{r}
goalkeepers <- str_count(players$player_positions, "GK")
sum(goalkeepers)
```

Indeed, there are no goalkeeper left.
Thus, while they are indisposable on the field, we could not say the same about their data, as it would reduce the accuracy of the classification of the other main positions.

### 2.3 Subset creation

We create two subsets: one with the market value in euro of the player, that we will use for the regression on the market price (we will call this subset 'players_regress') and a second subset that does not contain such attribute, since it is not useful for the position classification that we will perform with that.

```{r}
#subset WITH market value (for later)

players_copy <- as.data.frame(copy(players))

players_regress <-subset(players_copy, player_positions!="GK")

#subset WITHOUT market value
players_class <-subset(players_copy, player_positions!="GK", select = -value_eur)
```

### 2.4 Labelling

Some players play in multiple positions, but we only want to identify their main one, so we only keep that one.
Moreover, we turn the binary "preferred_foot" feature into a numerical type.

```{r}

#Keep only the main preferred position
players_class$player_positions<- word(players_class$player_positions, 1, sep = fixed(","))
unique(players_class$player_positions)

# Left foot is -1 and Right foot is 1. Basically one-hot encoding but we only have 2 categories so its easy

players_class$preferred_foot[players_class[,"preferred_foot"]== "Left"] <- as.numeric(-1)
players_class$preferred_foot[players_class[,"preferred_foot"]== "Right"] <- as.numeric(1)
players_class$preferred_foot <- as.numeric(players_class$preferred_foot)
```

We can see some of the positions available on the game.
Goalkeeper excluded, there are 26 positions, namely:

1.  LWB = Left Wing Back
2.  LB = Left Back
3.  LCB = Left Center Back
4.  CB = Center Back
5.  RCB = Right Center Back
6.  RB = Right Back
7.  RWB = Right Wing Back
8.  LDM = Left Defensive Midfield
9.  CDM = Center Defensive Midfield
10. RDM = Right Defensive Midfield
11. RCM = Right Center Midfield
12. CM = Center Midfield
13. LCM = Left Center Midfield
14. RAM = Right Attacking Midfield
15. CAM = Center Attacking Midfield
16. LAM = Left Attacking Midfield
17. LM = Left Midfield
18. RM = Right Midfield
19. LW = Left Winger
20. RW = Right Winger
21. LF = Left Forward
22. CF = Center Forward
23. RF = Right Striker
24. LS = Left Striker
25. ST = Striker
26. RS = Right Striker

Since 26 labels positions are clearly too many, we cluster them into nine classes of positions based on the area of action on the field.

***Note:*** Here we apply our "domain knowledge".

Below a picture where we can visually see how we are going to group the various positions to obtain the nine classes desired.

![field positions](field_positions.jpg)


```{r}

#central back
players_class$player_positions[players_class[,"player_positions"]== "LCB"|players_class[,"player_positions"]== "CB"|players_class[,"player_positions"]== "RCB"] <- "CB"

#left back
players_class$player_positions[players_class[,"player_positions"]== "LWB"|players_class[,"player_positions"]== "LB"]<-"LB"

#right back
players_class$player_positions[players_class[,"player_positions"]== "RWB"|players_class[,"player_positions"]== "RB"]<-"RB"

#central deffensive midfielder
players_class$player_positions[players_class[,"player_positions"]== "LDM"|players_class[,"player_positions"]== "CDM"|players_class[,"player_positions"]== "RDM"] <- "CDM"

#central midfielder
players_class$player_positions[players_class[,"player_positions"]== "LCM"|players_class[,"player_positions"]== "CM"|players_class[,"player_positions"]== "RCM"] <- "CM"

#central attacking midfielder
players_class$player_positions[players_class[,"player_positions"]== "LAM"|players_class[,"player_positions"]== "CAM"|players_class[,"player_positions"]== "RAM"] <- "CAM"

#left winger
players_class$player_positions[players_class[,"player_positions"]== "LM"|players_class[,"player_positions"]== "LW"|players_class[,"player_positions"]== "LF"] <- "LW"

#right winger
players_class$player_positions[players_class[,"player_positions"]== "RM"|players_class[,"player_positions"]== "RW"|players_class[,"player_positions"]== "RF"] <- "RW"

#striker
players_class$player_positions[players_class[,"player_positions"]== "LS"|players_class[,"player_positions"]== "CF"|players_class[,"player_positions"]== "RS"] <- "ST"

```

Lets take a look at the distribution of our labels.

```{r}

cat<- table(factor(players_class$player_positions))
pie(cat,
    col = hcl.colors(length(cat), "BluYl"))
```

Time to normalize the numerical values, as promised.
For that, we implement a simple re-scaling function, and we apply it on the whole dataframe.

```{r}

# normalization function
normalize <-function(x) { (x -min(x))/(max(x)-min(x))   }

# normalize 
players_norm <- as.data.frame(lapply(players_class[, c(3:42)], normalize))
head(players_norm,5)
```

### 2.5 Correlation matrix and feature selection

We create a correlation matrix.
It is big and maybe a bit hard to read, but R gives us the visually appealing option to group plotted features into highly correlated clusters.

```{r}

cormatrix <- cor(players_norm)
corrplot(cor(players_norm), method = 'shade', sig.level = 0.10, type = 'lower', order = 'hclust', title = "Correlation plot before feature selection")
```

Now, in order to reduce the number of features, we take away the ones that provide the data with the highest overall correlation.

```{r}

highcorr <- findCorrelation(cormatrix, cutoff=0.8)
highcorr
col2<-colnames(players_norm)
col2
col2<-col2[-highcorr]
corrplot.mixed(cor(players_norm[highcorr]), lower = "number", upper="shade", tl.pos = 'lt')
```

We take a look if we eliminated some of the dark spots from our correlation matrix.

```{r}
corrplot(cor(players_norm[col2]), type = 'lower',method = 'shade', order = 'hclust', title = "Correlation plot after feature selection")
players_f.selected <- players_norm[col2]  #features left
players_model <- subset(players_f.selected)

#we can add the positions back
players_model$player_positions <- c(players_class$player_positions)
```

```{r}
colnames(players_model)

```

We did.
Looks much better and ready for further investigation.

### 2.6 Individual feature investigation

We want to look at the individual distributions of each of the features left.
We fit violin plots, and put boxplots on top of them.

```{r}
# here we do the cool violin plots to check distributions

# set up the plotting layout as a 4x2 grid
par(mfrow = c(4, 2))

# create a combined boxplot and violin plot using ggplot2
ggplot(data = melt(players_f.selected[, 1:4]),  # convert data to long format (tabular form)
       aes(y = variable, x = value, fill = variable, alpha = 0.7)) +  # specify aesthetics
  geom_boxplot() +  # add boxplot layer
  geom_violin() +  # add violin plot layer
  scale_fill_manual(values = viridis(5)) +  # set fill colors using Viridis palette
  guides(fill = "none")  # remove legend for fill colors

```


```{r}

ggplot(data = melt(players_f.selected[,5:8]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

Weak foot is a discrete RV with values in 1-5.
Preferred foot is +/-1, as discussed above.
Still, as in real life, a significantly larger proportion of right-footed people.

```{r}

ggplot(data = melt(players_f.selected[,9:12]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

```{r}

ggplot(data = melt(players_f.selected[,13:16]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

```{r}

ggplot(data = melt(players_f.selected[,17:21]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

```{r}
ggplot(data = melt(players_f.selected[,22:26]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```
### 2.7 Principal Component Analysis

```{r}
players.pca<-prcomp(players_f.selected,center=TRUE, scale.=TRUE)
summary(players.pca)
```

We obtain 26 components. We want to visualize them.

```{r}
fviz_eig(players.pca, addlabels = TRUE)
```

The first 5 components account for 69.6% of the explained variance, while the first 2 for 50.8% of it. Now we want to see how our features project into the main 2D factor plane.

```{r}
fviz_pca_var(players.pca, labelsize = 2, alpha.var = 1.0, title = "Factor Plane for the FIFA 22 Data")
```

From the factor plane we can appreciate the relationship among particular attributes. Pace is positively correlated with agility and balance, which makes absolute sense since agility and a good balance are the key elements of the fastest players on the field. Another group of highly correlated features is the one that comprehends the physical attributes and aerial game: the taller and heaviest guys are usually the strongest ones and given their physical structure they are more likely to be also good headers. All these characteristics are the more desirable for a central defender (CB) and this is the reason why also aggression and marking awareness belong to the group.


## 3. Modelling - Multiclass classification

Now its finally time to dive into the actual modelling process.
We experiment and compare different classification algorithms.

### 3.1 Train-validation split

Classical split for training and testing models.
We keep the classical 70%-30% approach.

```{r}
## 70% of the sample size
smp_size <- floor(0.7 * nrow(players_model))

train_ind <- sample(seq_len(nrow(players_model)), size = smp_size)

train_with_label <- players_model[train_ind, ]
test_with_label <- players_model[-train_ind, ]

print('Train set size:')
print(dim(train_with_label))
print('Validation set size:')
print(dim(test_with_label))
```

We factorize the labels, so we can use them in our models.


```{r}
# factorize labels
train_y <- as.factor(train_with_label[,27])
test_y <- as.factor(test_with_label[,27])
# remove labels from sets
train <- train_with_label[1:(length(train_with_label)-1)]
test <- test_with_label[1:(length(test_with_label)-1)]
head(test)
```

Just to take a sneak peek, this is how the validation labels are roughly distributed on the factor plane.We notice that the factor plane separates some types of labels quite good (e.g. CB), some not.

```{r}
test.pca<-prcomp(test,center=TRUE, scale.=TRUE)
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = test_y,
                legend.title = "Players",
                title = "Classification of players")
```

### 3.2 Useful functions

Before we train any model, we want to create a function that computes accuracy, and one that selects the missclassified data so we can visualize it later on the factor plane.

```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

missclassified <- function(pred, label){
  l<- pred
  l[c(pred)==c(label)]<- 0
  return (as.factor(l))
}

#cross validation
ctrl <-  trainControl(method = "repeatedcv", number = 100, repeats = 10)
```

### 3.3 Knn

```{r}
##run knn function
class <- factor(c(train_y))

train <- train[1:(length(train)-1)]
test <- test[1:(length(test)-1)]

accuracy_vect <- c()
ks<- c()

for(k1 in seq(5,100,5)) {
    test_pred <-knn(train = train, test = test, cl = class, k = k1)
    accuracy_vect <- append(accuracy_vect,accuracy(table(test_y,test_pred)))
    ks <- append(ks, k1)
}

plot(ks, accuracy_vect, type = "p", col="blue", xlab="K's", ylab="accuracy", main="Accuracy vs K value plot")
abline(h = max(accuracy_vect), col = "darkorange", lty = 3)
```

We get the best k and its accuracy, represented by the orange line, namely k = 55, with 64.7% accuracy.

```{r}
print('The best K in our case is:')
print(ks[which.max(accuracy_vect)])
print('And it gives us an accuracy of:' )
print(accuracy_vect[which.max(accuracy_vect)])
```

Using the best k ( k=55) we generate a confusion matrix to check misslabeled data

```{r}
set.seed(42)
pred_knn <-knn(train = train, test = test, cl = class, k = 55)
```

```{r}
conf_mat_knn <- confusionMatrix(data=pred_knn, reference = test_y)
conf_mat_knn
```


### 3.4 Logistic regression

```{r}
set.seed(42)
modellr <- nnet::multinom( train_y~., data = train,trControl=crtl)
```

```{r}
set.seed(42)
pred_lr <- predict(modellr, test, type = "class")
accuracy(table(test_y, pred_lr))
```
The logistic regression performs better, with 69.5% accuracy.
Let's have a look at the confusion matrix.

```{r}
conf_mat <- confusionMatrix(data=pred_lr, reference = test_y)
conf_mat
```

### 3.5 Random Forrest

The hyperparameter we experiment with is 'mtry', that controls the number of variables randomly sampled as candidates for splitting at each tree node. Changing the number of trees does not do much, and from previous experimentation we realized that around 500 is the optimum value.

```{r}
acc = c()  # empty vector to store accuracy values
i = 5 

# Loop through values of i from 5 to 10
for (i in seq(5,10)) {
  # Train a random forest model with varying mtry values
  model_RF <- randomForest(train_y ~ ., data = train, ntree = 500, mtry = i, importance = TRUE)
  
  # Make predictions using the trained random forest model
  prediction_RF <- predict(model_RF, test, type = "class")
  
  # Calculate accuracy and store it in the 'acc' vector
  acc[i - 4] = mean(prediction_RF == test_y)  # Accuracy is calculated by comparing predicted and actual classes
}

# Plot accuracy values against the mtry values
plot(5:10, acc)

```


```{r}
acc
```

Randomly sampling 9 variables at each split gives the best accuracy with 68,4%.

```{r}
model_rf <- randomForest(train_y ~ ., data = train, ntree = 500, mtry = 9, importance = TRUE)
pred_rf <- predict(model_rf, test, type = "class")
```

```{r}
conf_mat <- confusionMatrix(data=pred_rf, reference = test_y)
conf_mat
```

### 3.6 SVM 

```{r}
svm1 <- svm(formula= train_y~., data=train, 
          type="C-classification", kernal="radial", 
          gamma=0.1, cost=10)
```


```{r}
pred_svm <- predict(svm1,test, type = "class")
accuracy(table(test_y, pred_svm))
```

We produce a summary of the model.

```{r}
summary(svm1)

```


```{r}
conf_mat <- confusionMatrix(data=pred_svm, reference = test_y)
conf_mat
```

### 3.7 Label Grouping

The accuracies obtained are decent but not great, and the confusion matrix clearly explains why.
Positions like CB, ST, LB, RB get classified really well.
On the opposite side, positions like CAM/CM and RW/LW get often confused.

The first missclassification is explainable with basic attributes of the role.
Centrer Attacking Midfielder (CAM) shares a lot of attacking characteristics with the Winger such as shooting and pace but also many with CM, like passing. This common attributes of course hindrance the task of the machine learning model.

It is a bit more tricky to detect why Left Winger and Right Winger get missclassified, when other positions that depends on the side (left or right), such Left Back and Right Back, are easily distinguished.
For LB and RB the preferred foot plays a big role, since it's hard to find a righty who plays on the left and viceversa, because they cross and tackle mostly with their dominant foot.
For RW and LW the distinction is less definable based on the preferred foot.
On one hand, a lot of righty players like to play as Left Winger so they can converge to the center to shoot with their strong foot. Same is true for lefty on RW.
On the other hand, many Wingers like to cross more, so they tend to do it with their preferred foot (LW with the left foot and RW with the right one).
So for the model of course it's really not an easy job to detect these differences that pertain to the single player style of play; and this problem explains the drop in accuracy for these positions.
In order to improve the accuracy of our classifiers, we group RW and LW together in a new position 'W = Winger' and we also group the CAM with CM in the class "CM = Center Midfielder".

```{r}

test_y2 <- test_y
levels(test_y2)[levels(test_y2) == "RW"| levels(test_y2) == "LW"] <- "W"
levels(test_y2)[levels(test_y2) == "CAM"| levels(test_y2) == "CM"] <- "CM"


train_y2 <- train_y
levels(train_y2)[levels(train_y2) == "RW"| levels(train_y2) == "LW"] <- "W"
levels(train_y2)[levels(train_y2) == "CAM"| levels(train_y2) == "CM"] <- "CM"

unique(test_y2)
```

```{r}
#plot pie chart again
cat<- table(factor(test_y2))
pie(cat, col = hcl.colors(length(cat), "BluYl"))
```

This is the new distribution of labels.
Now we reproduce the same experiments, expecting a hefty increase in accuracy, with the price of ablation.

### 3.7.1 Knn

```{r}

pred_knn2 <-knn(train = train, test = test, cl = train_y2, k = 51)
confusionMatrix(data=pred_knn2, reference = test_y2)
```

```{r}
accuracy(table(test_y2, pred_knn2))

```

### 3.7.2 Logistic Regression

```{r}
modellr2 <- nnet::multinom( train_y2~., data = train,trControl=crtl)
```

```{r}
pred_lr2 <- predict(modellr2, test, type = "class")
accuracy(table(test_y2, pred_lr2))
```

```{r}
confusionMatrix(data=pred_lr2, reference = test_y2)
```
### 3.6.2 Random Forrest

```{r}
 model_rf2 <- randomForest(train_y2 ~ ., data = train, ntree = 500, mtry = 8, importance = TRUE)
pred_rf2 <- predict(model_rf2, test, type = "class")
summary(model_rf2)
```

```{r}
accuracy(table(pred_rf2, test_y2))
```


```{r}
confusionMatrix(data=pred_rf2, reference = test_y2)
```

### 3.6.3 SVM

```{r}
model_svm2 <- svm(formula= train_y2~., data=train, 
          type="C-classification", kernal="radial", 
          gamma=0.1, cost=10)
pred_svm2 <- predict(model_svm2, test, type = "class")
```

```{r}
summary(model_svm2)
```

```{r}
accuracy(table(test_y2, pred_svm2))
```

```{r}
confusionMatrix(data=pred_svm2, reference = test_y2)
```
Overall we can appreciate a significant improvement in accuracy, on average of 8-9%, in all the models.
The model that performed the best was the Logistic Regression with 78.7%, followed in order by Random Forest (77.2%), SVM (75.3%) and KNN (72.9%). 
In today's Machine Learning standards these might not seem staggering result, but we cannot neglect the challenging task we tried to address. In such a complex sport where roles and attributes intertwine and change during the season or even in the same match, classify with precision the position of a player is not an easy job, because often a player cannot be confined to a single area of the field.

## 4. Regularization: Ridge regression and Lasso

In this part we explore two shrinkage methods, namely Ridge regression and Lasso, also known as penalized regression methods.
Through these techniques we can fit a model where all the predictors are contained, but some coefficient estimates are shrinked towards zero.

First we take the players_market and remove the player_positions column that we will not consider for this task.

```{r}
players_market <-subset(players_regress, select = -player_positions)
head(players_market)
```

```{r}
players_market$value_eur <- as.integer(players_market$value_eur)
# Left foot is -1 and Right foot is 1. Basically one-hot encoding but we only have 2 categories so its easy
players_market$preferred_foot[players_market[,"preferred_foot"]== "Left"] <- as.numeric(-1)
players_market$preferred_foot[players_market[,"preferred_foot"]== "Right"] <- as.numeric(1)
players_market$preferred_foot <- as.numeric(players_market$preferred_foot)
head(players_market)
```

```{r}
#norm
# normalization function
normalize <-function(x) { (x -min(x))/(max(x)-min(x))   }
colnames(players_market)
# normalize 
players_market_norm <- as.data.frame(lapply(players_market[, c(2:44)], normalize))
head(players_market_norm,5)
```

```{r}
attach(players_market_norm)
X <- model.matrix(value_eur ~.,players_market_norm)
y <- value_eur
```

### 4.1 Ridge Regression

Ridge uses quadratic shrinking.

```{r}
#ridge regression
grid.ridge <-10^seq(-4,2,length=100)
ridge.mod<-glmnet(X,y,alpha = 0,lambda = grid.ridge)
plot(ridge.mod, xvar="lambda", label= TRUE)
```

The plot illustrates how much the coefficients are penalized for different values of lambda.
Notice none of the coefficients are forced to be zero.

Looking at the plot the features:

[2]age has a negative impact on th market value, which makes absolute sense since the older the player the less money he will be worth.
On the other hand, [4] international_reputation, [26] movement acceleration and [3] potential have a big positive impact on the market price

```{r}
colnames(players_market)
```

```{r}
# select n/2 observations for training set
train <- sample(1:nrow(X), nrow(X)/2)
test <- (-train)
y.test <- y[test]
```

```{r}
# fit ridge regression on the training set
ridge.mod <- glmnet(X[train, ], y[train], alpha = 0,
                    lambda = grid.ridge, thresh = 1e-12)
```

We estimate the test MSE for one lambda value, e.g lambda = 9

```{r}
#We estimate the test MSE for one lambda value, e.g lambda = 9
ridge.pred <- predict(ridge.mod, s = 9, newx = X[test, ], type="response")
mean((ridge.pred - y.test)^2)
```

let's see the coefficients for lambda = 9

```{r}
predict(ridge.mod, s = 9, exact = TRUE, type = "coefficients",
        x = X[train, ], y = y[train])[1:41, ]
```

We use cross-validation to choose the value of lambda

```{r}
cv.out.ridge <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
cv.out.ridge$lambda[1:10]
```

```{r}
summary(cv.out.ridge$lambda)
```

```{r}
# The mean cross-validated error
cv.out.ridge$cvm[1:10]
```

The following plot shows the cross-validation curve (red dotted line) with upper and lower standard deviation curves along the lambda sequence (error bars).

Two special values along the lambda sequence are indicated by the vertical dotted lines:

lambda.min is the value of lambda that gives minimum mean cross-validated error, while lambda.1se is the value of lambda that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.

```{r}
plot(cv.out.ridge)
```

```{r}
# identify the best lambda value
i.bestlam <- which.min(cv.out.ridge$cvm)
i.bestlam 
```

```{r}
bestlam <- cv.out.ridge$lambda[i.bestlam]
bestlam
```

```{r}
# mean cross-validated error for best lambda
cv.out.ridge$cvm[i.bestlam]
```

```{r}
# estimate the test MSE 
ridge.pred <- predict(ridge.mod, s = bestlam,
                      newx = X[test, ])
#MSE
mean((ridge.pred - y.test)^2)
```

```{r}
# fit the coefficient with lambda=bestlam on all the data
out <- glmnet(X, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)
```

```{r}
# residual sum of squares
ridge.rss <- sum((ridge.pred - y.test)^2)
ridge.rss
```

Let's compute the R squared.
The R squared is a statistical measure of how well the regression predictions approximate the real data points.
An R2 of 1 indicates that the regression predictions perfectly fit the data

```{r}
ridge.tss <- sum((y.test - mean(y.test)) ^ 2)  ## total sum of squares
ridge.rsq <- 1 - ridge.rss/ridge.tss  # R squared
ridge.rsq
```

### 4.2 Lasso Regularization

Lasso uses absolute-value shrinking.

```{r}
grid.lasso<-10^seq(-0.5,-8,length=100)
lasso.mod<-glmnet(X,y,alpha = 1,lambda = grid.lasso)
plot(lasso.mod, xvar="lambda", label = TRUE)
```

In the Lasso plot we can notice that some coefficients are forced to be zero.
Moreover, it is clear once again the impact of international_reputation, acceleration, potential and age on the estimation of a player market price.

We use cross-validation to choose the value of lambda

```{r}
cv.out.lasso <- cv.glmnet(X[train, ], y[train], alpha = 1, nfold=10)
cv.out.lasso$lambda[1:10]
```

```{r}
# apply lasso to the training set 
lasso.mod <- glmnet(X[train,], y[train], alpha=1, lambda=grid.lasso)
```

```{r}
# apply 10fold cross-validation to the training set
cv.out.lasso <- cv.glmnet(X[train,], y[train], alpha=1)
plot(cv.out.lasso)
```

```{r}
# estimate test MSE
bestlam <- cv.out.lasso$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=X[test,])
mean((lasso.pred-y.test)^2)
```

```{r}
# fit the model with best-lambda on all the data
lasso.coef <- predict(lasso.mod,type="coefficients",s=bestlam)[1:44,]
lasso.coef
```

We check which coefficients were forced to zero

```{r}
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0])
which(lasso.coef==0)
```

Shooting, passing, dribbling, defending, physic, attacking_volleys, skill_ball_control and defending_sliding_tackle were forced to be zero.

```{r}
# residual sum of squares
lasso.rss <- sum((lasso.pred - y.test)^2)
lasso.rss
```

```{r}
lasso.tss <- sum((y.test - mean(y.test)) ^ 2)  ## total sum of squares
lasso.rsq <- 1 - lasso.rss/lasso.tss  # R squared
lasso.rsq
```

```{r}
#ridge.rss vs lasso.rss
ridge.rss
lasso.rss
```

```{r}
#ridge.rsq vs lasso.rsq
ridge.rsq
lasso.rsq
```

Overall, we can say Lasso regression performs slightly better than Ridge Regression, given the higher R squared and lower Residual Sum of Squares.
From both models is clear how age and agility are the features that mostly impact the market value (negatively and positively, respectively), together with the international reputation that has the biggest positive influence of them all.
The results show that the models explain around 54% of the variance for the market value.
R squared value between 0.5 and 0.7 is considered a moderate effect size.


### 5. Conclusion and further research

All in all, position classification is possible for some distinct areas of the football field, but for some specific ones is quite impossible, in the case of multiclass classification.

On one hand, football is a very heterogeneous sport and often the values of the attributes cannot explain as a whole the position of a player since his style of play heavily influence how the role is interpreted and consequently where exactly the player acts on the field.
On the other hand, we would also like to believe that with sufficient data, even effective positioning of real players could be calculated.

As regard players' market price, an R squared of 0.54 is a decent value, but not remarkable.
Of course age plays a big role, since young footballers have way larger margins of improvement; but it is also true that aspects like the contract expiry, which was not provided, have a significant effect in real life.
When the date of expiry of a player approaches, the club is more inclined to agree even a lower price in order not to lose this athlete for free.
The feature with the undisputed heavier positive impact is international_reputation and this come as no surprise.
Football players that have a huge fan base (for example on social media) have a higher price since the club can profit from his visibility, that will result in more supporters and more revenue.
Market value is the result of plenty of different information, some of which are very difficult to add among the features.
For example, the current form of a player greatly influence his value: a striker scoring for the previous 9 consecutive matches will surely see his value skyrocket.
With additional information like these, the models would definitely improve their performance.
